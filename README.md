## Reflection

### 1. Key differences between unary, server-streaming, and bi-directional streaming RPC methods  
Unary RPCs involve a single request from the client and a single response from the server. They are straightforward to implement, have minimal overhead, and are ideal for one-off operations like fetching a record or performing a calculation. Server-streaming RPCs allow the client to send one request and then receive a sequence of responses, which makes them suitable for scenarios where large datasets or continuous updates need to be pushed (for example, streaming logs or real-time market data). Bi-directional streaming RPCs open a channel where both client and server can send arbitrary sequences of messages independently; this model supports complex interactive workflows such as chat applications, multiplayer games, or collaborative editing. Each pattern trades off simplicity, resource usage, and real-time capabilities, so choosing the right one depends on your application's data volume, latency requirements, and interaction model.

### 2. Potential security considerations in Rust gRPC services  
When securing a Rust gRPC service, authentication must ensure that only legitimate clients can connect; this often involves bearer tokens (JWTs) or mTLS. Authorization is a secondary layer where authenticated identities are granted or denied access to specific RPC methods, which can be enforced via request interceptors or middleware. Data encryption in transit is typically handled by enabling TLS on the HTTP/2 transport, protecting against eavesdropping and man-in-the-middle attacks. Additional measures include input validation to prevent malformed messages from causing crashes or logic errors, and resource limits (like maximum message size and stream timeouts) to mitigate denial-of-service attacks. Finally, audit logging and monitoring should capture each RPC call and its metadata for forensic analysis and compliance.

### 3. Challenges when handling bidirectional streaming in Rust gRPC  
Bidirectional streaming introduces complexity in coordinating asynchronous I/O on both sides of the connection; the server must read and write concurrently without blocking. Backpressure becomes a major concern, since the rate at which the client sends messages may outpace the server’s ability to process and respond, leading to channel saturation or dropped messages. Error handling is tricky: a failure in one half of the stream (e.g., client decode error) can require tearing down the entire stream or gracefully notifying the other side. Lifecycle management also grows more complex, as the server needs to detect client disconnects and clean up resources promptly to avoid memory leaks. Testing and debugging streaming logic can be more difficult than unary calls, because reproducing timing and ordering issues often demands specialized test harnesses.

### 4. Advantages and disadvantages of using `tokio_stream::wrappers::ReceiverStream`  
`ReceiverStream` provides a simple adapter for converting a Tokio MPSC channel into a `Stream` that tonic can return in a server-streaming response. This reduces boilerplate and leverages well-understood channel semantics for buffering and backpressure. Because it is built on top of Tokio, it integrates smoothly with other async tasks and timers for pacing or timeouts. However, using a fixed channel buffer size can lead to blocked senders if the client consumes messages slowly, and tuning this buffer requires workload-specific benchmarking. Additionally, `ReceiverStream` does not implement advanced flow control policies beyond the channel’s basic behavior, so complex coordination logic (e.g., pausing and resuming producers dynamically) must be managed manually.

### 5. Structuring Rust gRPC code for reuse and modularity  
To promote maintainability, one can split code into a **`common`** crate that holds shared types and helper functions, and separate **`server`** and **`client`** crates for their respective implementations. Organizing each service implementation into its own module (e.g., `payment`, `transaction`, `chat`) allows adding new RPCs or refactoring one service without affecting others. Defining traits for cross-cutting concerns—such as logging, metrics, or authentication—facilitates code reuse via generic wrappers or tonic interceptors. Shared configuration (like TLS settings, timeouts, and retry policies) can live in a central `config` module, making it easy to apply consistent policies across services. Finally, using a build script (`build.rs`) to generate and recompile `.proto` definitions ensures the client and server are always in sync with the schema.

### 6. Extending `MyPaymentService` for complex logic  
A production-grade payment service must validate request fields (e.g., ensure `amount` is positive and `user_id` conforms to expected format) before processing. It should integrate with an external payment gateway or database transaction manager, handling retries, timeouts, and idempotency keys to avoid duplicate charges. Audit trails and logging would capture every request and response, along with relevant metadata for compliance and debugging. Error paths (such as payment failures or network errors) need structured handling, returning informative status codes and messages to clients. Finally, asynchronous notifications (for example, via event streams or webhooks) can inform other systems of payment outcomes in real time.

### 7. Impact of gRPC on distributed systems architecture  
Adopting gRPC shifts distributed systems toward strongly typed, contract-driven interfaces defined in `.proto` files, which enables automatic code generation across polyglot environments. HTTP/2’s multiplexed streams reduce connection overhead, improving performance for microservices that communicate frequently. gRPC’s built-in streaming and flow control primitives simplify real-time data pipelines compared to rolling your own socket or WebSocket logic. However, it introduces dependencies on gRPC-aware load balancers, service meshes, and monitoring tools that understand HTTP/2. Interoperability with legacy REST clients can require sidecar proxies or gateway layers to translate between gRPC and JSON/HTTP1.1.

### 8. Advantages and disadvantages of HTTP/2 vs HTTP/1.1 (or WebSocket) for APIs  
HTTP/2 natively supports multiplexed, bidirectional streams, header compression (HPACK), and prioritization, which collectively improve latency and throughput for RPC workloads. It relies on TLS in most implementations, enhancing security but complicating setup compared to plain HTTP/1.1. HTTP/1.1 with WebSocket can approximate streaming but requires a separate upgrade handshake and lacks standardized RPC framing. WebSocket traffic is harder to inspect with traditional HTTP tooling, whereas HTTP/2 remains an HTTP-based protocol with mature ecosystem support. On the other hand, HTTP/1.1 endpoints and JSON payloads enjoy broader client compatibility without requiring specialized gRPC libraries.

### 9. REST request-response vs gRPC bidirectional streaming  
REST APIs follow a fixed request-response cycle: the client issues a request and waits for a response, making real-time updates difficult without polling or WebSockets. gRPC’s streaming model allows servers to push updates whenever they occur and clients to stream data back in the same connection, eliminating polling overhead. This leads to lower latency for interactive applications like chat or IoT telemetry. REST’s simplicity and RESTful semantics (CRUD over resources) are easy to understand, cache, and debug with HTTP tools. Conversely, gRPC’s binary framing and multiplexing demand specialized clients but yield higher efficiency and stronger type safety.

### 10. Protocol Buffers vs JSON payloads  
Protocol Buffers enforce a **schema-first** approach, where `.proto` definitions generate code in multiple languages, ensuring strict type checking and backward-compatible schema evolution. The binary format is compact and fast to serialize/deserialize, which is critical for high-performance services. However, it is not human-readable, making ad-hoc debugging or manual API exploration more difficult. JSON is schema-less and human-friendly, allowing clients to send arbitrary fields without recompilation, but this flexibility can lead to runtime errors and inconsistent payload structures. JSON payloads incur larger message sizes and slower parsers, impacting network and CPU overhead under heavy load.
